{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('BDASProj').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YEAR',\n",
       " 'SAMPLE',\n",
       " 'SERIAL',\n",
       " 'CBSERIAL',\n",
       " 'HHWT',\n",
       " 'STATEICP',\n",
       " 'GQ',\n",
       " 'PERNUM',\n",
       " 'PERWT',\n",
       " 'SEX',\n",
       " 'AGE',\n",
       " 'MARST',\n",
       " 'RACE',\n",
       " 'RACED',\n",
       " 'EDUC',\n",
       " 'EDUCD',\n",
       " 'EMPSTAT',\n",
       " 'EMPSTATD',\n",
       " 'OCC',\n",
       " 'INCWAGE']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading it in\n",
    "df = spark.read.csv('/home/ubuntu/BDASProj/usa_00007.csv',header=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----+----+----+-------+\n",
      "|SEX|AGE|MARST|RACE|EDUC| OCC|INCWAGE|\n",
      "+---+---+-----+----+----+----+-------+\n",
      "|  2| 31|    6|   1|  10| 350|  38500|\n",
      "|  2| 37|    4|   2|   6| 230|  18000|\n",
      "|  1| 21|    6|   2|   7|4620|  15000|\n",
      "|  1| 20|    6|   2|   7|4120|   1200|\n",
      "|  1| 61|    2|   1|  10|1410| 160000|\n",
      "|  1| 62|    1|   2|  10| 430| 100000|\n",
      "|  2| 58|    1|   2|  10|4710| 344000|\n",
      "|  1| 30|    6|   2|  10|4710| 120000|\n",
      "|  1| 26|    6|   2|  10|4850|  50000|\n",
      "|  2| 55|    6|   1|   7|4500|      0|\n",
      "|  1| 54|    4|   1|  10|4840|  56000|\n",
      "|  1| 36|    4|   1|   6|9130|  24700|\n",
      "|  1| 50|    1|   1|  11|2200| 344000|\n",
      "|  2| 47|    1|   1|   6|2340|  10000|\n",
      "|  2| 47|    1|   2|   8|4510|  15000|\n",
      "|  1| 53|    1|   2|   8|4760|  30000|\n",
      "|  2| 26|    6|   1|   7|5220|  25000|\n",
      "|  1| 22|    6|   2|   7|8965|  29500|\n",
      "|  1| 48|    1|   1|   6|3740|  65000|\n",
      "|  2| 49|    1|   1|  10|3600|  65000|\n",
      "+---+---+-----+----+----+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['SEX', 'AGE', 'MARST', 'RACE', 'EDUC', 'OCC', 'INCWAGE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deleting unwanted columns, and setting up df schema\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "#deleting columns and showing new df\n",
    "df2 = df.select([c for c in df.columns if c not in {'YEAR','SAMPLE',\"SERIAL\",\"CBSERIAL\",\"HHWT\",\"STATEICP\",\"GQ\",\"PERNUM\",\"PERWT\",\"RACED\",\"EDUCD\",\"EMPSTAT\",\"EMPSTATD\"}])\n",
    "df2.show()\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------------+-----+--------------+----+-------+\n",
      "|   SEX|AGE|              MARST| RACE|          EDUC| OCC|INCWAGE|\n",
      "+------+---+-------------------+-----+--------------+----+-------+\n",
      "|Female| 31|NeverMarried-Single|White|4thYearCollege| 350|  38500|\n",
      "|Female| 37|          Separated|Black|    HighSchool| 230|  18000|\n",
      "|  Male| 21|NeverMarried-Single|Black|1stYearCollege|4620|  15000|\n",
      "|  Male| 20|NeverMarried-Single|Black|1stYearCollege|4120|   1200|\n",
      "|  Male| 61|            Married|White|4thYearCollege|1410| 160000|\n",
      "|  Male| 62|            Married|Black|4thYearCollege| 430| 100000|\n",
      "|Female| 58|            Married|Black|4thYearCollege|4710| 344000|\n",
      "|  Male| 30|NeverMarried-Single|Black|4thYearCollege|4710| 120000|\n",
      "|  Male| 26|NeverMarried-Single|Black|4thYearCollege|4850|  50000|\n",
      "|Female| 55|NeverMarried-Single|White|1stYearCollege|4500|      0|\n",
      "|  Male| 54|          Separated|White|4thYearCollege|4840|  56000|\n",
      "|  Male| 36|          Separated|White|    HighSchool|9130|  24700|\n",
      "|  Male| 50|            Married|White|5+YearsCollege|2200| 344000|\n",
      "|Female| 47|            Married|White|    HighSchool|2340|  10000|\n",
      "|Female| 47|            Married|Black|2ndYearCollege|4510|  15000|\n",
      "|  Male| 53|            Married|Black|2ndYearCollege|4760|  30000|\n",
      "|Female| 26|NeverMarried-Single|White|1stYearCollege|5220|  25000|\n",
      "|  Male| 22|NeverMarried-Single|Black|1stYearCollege|8965|  29500|\n",
      "|  Male| 48|            Married|White|    HighSchool|3740|  65000|\n",
      "|Female| 49|            Married|White|4thYearCollege|3600|  65000|\n",
      "+------+---+-------------------+-----+--------------+----+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+\n",
      "|                RACE|\n",
      "+--------------------+\n",
      "|Native-IndianOrAl...|\n",
      "|             Chinese|\n",
      "|            Japanese|\n",
      "|OtherAsianOrPacif...|\n",
      "|               White|\n",
      "|               Black|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change values within columns (all except OCC)\n",
    "sexDict = {'1':'Male','2':'Female'} \n",
    "df2 = df2.na.replace(sexDict,1,\"SEX\")\n",
    "\n",
    "mDict = {'1':'Married','2':'Married','3':'Separated','4':'Separated','5':'Widowed','6':'NeverMarried-Single'}\n",
    "df2 = df2.na.replace(mDict,1,\"MARST\")\n",
    "\n",
    "rDict = {'1': \"White\", '2': \"Black\", '3': \"Native-IndianOrAlaskan\", '4': \"Chinese\", '5': \"Japanese\", '6': \"OtherAsianOrPacificIslander\"}\n",
    "df2 = df2.na.replace(rDict,1,\"RACE\")\n",
    "\n",
    "eDict = {'0': \"NoSchooling\", '1': \"NurseryToKinderGarten\", '2': \"MiddleSchool\", '3': \"HighSchool\", '4': \"HighSchool\", '5': \"HighSchool\", '6': \"HighSchool\",'7': \"1stYearCollege\", '8': \"2ndYearCollege\", '9': \"3rdYearCollege\", '10': \"4thYearCollege\", '11': \"5+YearsCollege\"}\n",
    "df2 = df2.na.replace(eDict,1,\"EDUC\")\n",
    "df2.show()\n",
    "\n",
    "#removing 7,8,9 from RACE (we dont consider those race codes)\n",
    "df2 = df2.filter('RACE not in (\"7\",\"8\",\"9\")')\n",
    "df2.select('RACE').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then retaining only non-zero INCWAGEs\n",
    "df2 = df2.filter(df2['INCWAGE']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------------+-----+--------------+------+-------+---------+\n",
      "|   SEX|AGE|              MARST| RACE|          EDUC|   OCC|INCWAGE|OCCRecode|\n",
      "+------+---+-------------------+-----+--------------+------+-------+---------+\n",
      "|Female| 31|NeverMarried-Single|White|4thYearCollege| 350.0|  38500|      0.0|\n",
      "|Female| 37|          Separated|Black|    HighSchool| 230.0|  18000|      0.0|\n",
      "|  Male| 21|NeverMarried-Single|Black|1stYearCollege|4620.0|  15000|      9.0|\n",
      "|  Male| 20|NeverMarried-Single|Black|1stYearCollege|4120.0|   1200|      9.0|\n",
      "|  Male| 61|            Married|White|4thYearCollege|1410.0| 160000|      2.0|\n",
      "|  Male| 62|            Married|Black|4thYearCollege| 430.0| 100000|      0.0|\n",
      "|Female| 58|            Married|Black|4thYearCollege|4710.0| 344000|     10.0|\n",
      "|  Male| 30|NeverMarried-Single|Black|4thYearCollege|4710.0| 120000|     10.0|\n",
      "|  Male| 26|NeverMarried-Single|Black|4thYearCollege|4850.0|  50000|     10.0|\n",
      "|  Male| 54|          Separated|White|4thYearCollege|4840.0|  56000|     10.0|\n",
      "|  Male| 36|          Separated|White|    HighSchool|9130.0|  24700|     13.0|\n",
      "|  Male| 50|            Married|White|5+YearsCollege|2200.0| 344000|      5.0|\n",
      "|Female| 47|            Married|White|    HighSchool|2340.0|  10000|      5.0|\n",
      "|Female| 47|            Married|Black|2ndYearCollege|4510.0|  15000|      9.0|\n",
      "|  Male| 53|            Married|Black|2ndYearCollege|4760.0|  30000|     10.0|\n",
      "|Female| 26|NeverMarried-Single|White|1stYearCollege|5220.0|  25000|     11.0|\n",
      "|  Male| 22|NeverMarried-Single|Black|1stYearCollege|8965.0|  29500|     13.0|\n",
      "|  Male| 48|            Married|White|    HighSchool|3740.0|  65000|      8.0|\n",
      "|Female| 49|            Married|White|4thYearCollege|3600.0|  65000|      7.0|\n",
      "|  Male| 60|            Married|White|5+YearsCollege|1020.0| 114000|      2.0|\n",
      "+------+---+-------------------+-----+--------------+------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------+\n",
      "|OCCRecode|\n",
      "+---------+\n",
      "|      8.0|\n",
      "|      0.0|\n",
      "|      7.0|\n",
      "|      1.0|\n",
      "|      4.0|\n",
      "|     11.0|\n",
      "|     14.0|\n",
      "|      3.0|\n",
      "|      2.0|\n",
      "|     10.0|\n",
      "|     13.0|\n",
      "|      6.0|\n",
      "|      5.0|\n",
      "|      9.0|\n",
      "|     12.0|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Monumental Task: Reclassifying OCC\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "#first convert stringtype of occrecode to doubletype\n",
    "df2 = df2.withColumn(\"OCC\", df2[\"OCC\"].cast(DoubleType()))\n",
    "\n",
    "#then reclassify\n",
    "bucketizer = Bucketizer(splits=[0,741,951,1966,2060,2161,2551,2921,3656,3956,4651,4966,5941,6131,9751,9921],\n",
    "                       inputCol=\"OCC\",outputCol=\"OCCRecode\")\n",
    "df2_buck = bucketizer.setHandleInvalid(\"keep\").transform(df2)\n",
    "\n",
    "#showing the occrecode column, as well as the number of distinct values (Showing it Worked!)\n",
    "df2_buck.show()\n",
    "df2_buck.select(\"OCCRecode\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|     OCCRecode|\n",
      "+--------------+\n",
      "|     Education|\n",
      "|         Sales|\n",
      "|    Healthcare|\n",
      "|  PublicSector|\n",
      "|       Finance|\n",
      "|         Media|\n",
      "|          STEM|\n",
      "|   Hospitality|\n",
      "|Administration|\n",
      "|           Law|\n",
      "|      Business|\n",
      "|        Trades|\n",
      "+--------------+\n",
      "\n",
      "+------+---+-------------------+-----+--------------+------+-------+--------------+\n",
      "|   SEX|AGE|              MARST| RACE|          EDUC|   OCC|INCWAGE|     OCCRecode|\n",
      "+------+---+-------------------+-----+--------------+------+-------+--------------+\n",
      "|Female| 31|NeverMarried-Single|White|4thYearCollege| 350.0|  38500|      Business|\n",
      "|Female| 37|          Separated|Black|    HighSchool| 230.0|  18000|      Business|\n",
      "|  Male| 21|NeverMarried-Single|Black|1stYearCollege|4620.0|  15000|   Hospitality|\n",
      "|  Male| 20|NeverMarried-Single|Black|1stYearCollege|4120.0|   1200|   Hospitality|\n",
      "|  Male| 61|            Married|White|4thYearCollege|1410.0| 160000|          STEM|\n",
      "|  Male| 62|            Married|Black|4thYearCollege| 430.0| 100000|      Business|\n",
      "|Female| 58|            Married|Black|4thYearCollege|4710.0| 344000|         Sales|\n",
      "|  Male| 30|NeverMarried-Single|Black|4thYearCollege|4710.0| 120000|         Sales|\n",
      "|  Male| 26|NeverMarried-Single|Black|4thYearCollege|4850.0|  50000|         Sales|\n",
      "|  Male| 54|          Separated|White|4thYearCollege|4840.0|  56000|         Sales|\n",
      "|  Male| 36|          Separated|White|    HighSchool|9130.0|  24700|        Trades|\n",
      "|  Male| 50|            Married|White|5+YearsCollege|2200.0| 344000|     Education|\n",
      "|Female| 47|            Married|White|    HighSchool|2340.0|  10000|     Education|\n",
      "|Female| 47|            Married|Black|2ndYearCollege|4510.0|  15000|   Hospitality|\n",
      "|  Male| 53|            Married|Black|2ndYearCollege|4760.0|  30000|         Sales|\n",
      "|Female| 26|NeverMarried-Single|White|1stYearCollege|5220.0|  25000|Administration|\n",
      "|  Male| 22|NeverMarried-Single|Black|1stYearCollege|8965.0|  29500|        Trades|\n",
      "|  Male| 48|            Married|White|    HighSchool|3740.0|  65000|  PublicSector|\n",
      "|Female| 49|            Married|White|4thYearCollege|3600.0|  65000|    Healthcare|\n",
      "|  Male| 60|            Married|White|5+YearsCollege|1020.0| 114000|          STEM|\n",
      "+------+---+-------------------+-----+--------------+------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#continuing...\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "t = {0.0:'Business',1.0:'Finance',2.0:'STEM',3.0:'PublicSector',4.0:'Law',\n",
    "     5.0:'Education',6.0:'Media',7.0:'Healthcare',8.0:'PublicSector',9.0:\n",
    "     'Hospitality',10.0:'Sales',11.0:'Administration',12.0:'STEM',13.0:'Trades',14.0:'PublicSector'}\n",
    "udf_foo = udf(lambda x: t[x], StringType())\n",
    "df3 = df2_buck.withColumn('OCCRecode',udf_foo(\"OCCRecode\"))\n",
    "\n",
    "#showing the new recoded occupations\n",
    "df3.select(\"OCCRecode\").distinct().show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------------+-----+--------------+------+-------+--------------+--------------+\n",
      "|   SEX|AGE|              MARST| RACE|          EDUC|   OCC|INCWAGE|     OCCRecode|     M_MedWage|\n",
      "+------+---+-------------------+-----+--------------+------+-------+--------------+--------------+\n",
      "|Female| 31|NeverMarried-Single|White|4thYearCollege| 350.0|  38500|      Business|      Business|\n",
      "|Female| 37|          Separated|Black|    HighSchool| 230.0|  18000|      Business|      Business|\n",
      "|  Male| 21|NeverMarried-Single|Black|1stYearCollege|4620.0|  15000|   Hospitality|   Hospitality|\n",
      "|  Male| 20|NeverMarried-Single|Black|1stYearCollege|4120.0|   1200|   Hospitality|   Hospitality|\n",
      "|  Male| 61|            Married|White|4thYearCollege|1410.0| 160000|          STEM|          STEM|\n",
      "|  Male| 62|            Married|Black|4thYearCollege| 430.0| 100000|      Business|      Business|\n",
      "|Female| 58|            Married|Black|4thYearCollege|4710.0| 344000|         Sales|         Sales|\n",
      "|  Male| 30|NeverMarried-Single|Black|4thYearCollege|4710.0| 120000|         Sales|         Sales|\n",
      "|  Male| 26|NeverMarried-Single|Black|4thYearCollege|4850.0|  50000|         Sales|         Sales|\n",
      "|  Male| 54|          Separated|White|4thYearCollege|4840.0|  56000|         Sales|         Sales|\n",
      "|  Male| 36|          Separated|White|    HighSchool|9130.0|  24700|        Trades|        Trades|\n",
      "|  Male| 50|            Married|White|5+YearsCollege|2200.0| 344000|     Education|     Education|\n",
      "|Female| 47|            Married|White|    HighSchool|2340.0|  10000|     Education|     Education|\n",
      "|Female| 47|            Married|Black|2ndYearCollege|4510.0|  15000|   Hospitality|   Hospitality|\n",
      "|  Male| 53|            Married|Black|2ndYearCollege|4760.0|  30000|         Sales|         Sales|\n",
      "|Female| 26|NeverMarried-Single|White|1stYearCollege|5220.0|  25000|Administration|Administration|\n",
      "|  Male| 22|NeverMarried-Single|Black|1stYearCollege|8965.0|  29500|        Trades|        Trades|\n",
      "|  Male| 48|            Married|White|    HighSchool|3740.0|  65000|  PublicSector|  PublicSector|\n",
      "|Female| 49|            Married|White|4thYearCollege|3600.0|  65000|    Healthcare|    Healthcare|\n",
      "|  Male| 60|            Married|White|5+YearsCollege|1020.0| 114000|          STEM|          STEM|\n",
      "+------+---+-------------------+-----+--------------+------+-------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now we add a column for male median grouped by OCCRecode\n",
    "df3 = df3.withColumn(\"M_MedWage\",df3[\"OCCRecode\"])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+\n",
      "|   SEX|AGE|              MARST| RACE|          EDUC|INCWAGE|     OCCRecode|M_MedWage|\n",
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+\n",
      "|Female| 31|NeverMarried-Single|White|4thYearCollege|  38500|      Business|    80000|\n",
      "|Female| 37|          Separated|Black|    HighSchool|  18000|      Business|    80000|\n",
      "|  Male| 21|NeverMarried-Single|Black|1stYearCollege|  15000|   Hospitality|    18000|\n",
      "|  Male| 20|NeverMarried-Single|Black|1stYearCollege|   1200|   Hospitality|    18000|\n",
      "|  Male| 61|            Married|White|4thYearCollege| 160000|          STEM|    73500|\n",
      "|  Male| 62|            Married|Black|4thYearCollege| 100000|      Business|    80000|\n",
      "|Female| 58|            Married|Black|4thYearCollege| 344000|         Sales|    42000|\n",
      "|  Male| 30|NeverMarried-Single|Black|4thYearCollege| 120000|         Sales|    42000|\n",
      "|  Male| 26|NeverMarried-Single|Black|4thYearCollege|  50000|         Sales|    42000|\n",
      "|  Male| 54|          Separated|White|4thYearCollege|  56000|         Sales|    42000|\n",
      "|  Male| 36|          Separated|White|    HighSchool|  24700|        Trades|    36000|\n",
      "|  Male| 50|            Married|White|5+YearsCollege| 344000|     Education|    47500|\n",
      "|Female| 47|            Married|White|    HighSchool|  10000|     Education|    47500|\n",
      "|Female| 47|            Married|Black|2ndYearCollege|  15000|   Hospitality|    18000|\n",
      "|  Male| 53|            Married|Black|2ndYearCollege|  30000|         Sales|    42000|\n",
      "|Female| 26|NeverMarried-Single|White|1stYearCollege|  25000|Administration|    32000|\n",
      "|  Male| 22|NeverMarried-Single|Black|1stYearCollege|  29500|        Trades|    36000|\n",
      "|  Male| 48|            Married|White|    HighSchool|  65000|  PublicSector|    46850|\n",
      "|Female| 49|            Married|White|4thYearCollege|  65000|    Healthcare|    67000|\n",
      "|  Male| 60|            Married|White|5+YearsCollege| 114000|          STEM|    73500|\n",
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#then putting in values for median wage\n",
    "medWageMap={\n",
    "    'Administration':'32000',\n",
    "    'Business':'80000',\n",
    "    'Education':'47500',\n",
    "    'Healthcare':'67000',\n",
    "    'Hospitality':'18000',\n",
    "    'Law':'85000',\n",
    "    'Media':'35000',\n",
    "    'PublicSector':'46850',\n",
    "    'STEM':'73500',\n",
    "    'Sales':'42000',\n",
    "    'Trades':'36000',\n",
    "    'Finance':'97000'\n",
    "}\n",
    "df3 = df3.na.replace(medWageMap,1,\"M_MedWage\")\n",
    "df3 = df3.drop(\"OCC\")\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+-------------------+\n",
      "|   SEX|AGE|              MARST| RACE|          EDUC|INCWAGE|     OCCRecode|M_MedWage|        PayGapRatio|\n",
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+-------------------+\n",
      "|Female| 31|NeverMarried-Single|White|4thYearCollege|  38500|      Business|    80000|            0.48125|\n",
      "|Female| 37|          Separated|Black|    HighSchool|  18000|      Business|    80000|              0.225|\n",
      "|Female| 58|            Married|Black|4thYearCollege| 344000|         Sales|    42000|   8.19047619047619|\n",
      "|Female| 47|            Married|White|    HighSchool|  10000|     Education|    47500|0.21052631578947367|\n",
      "|Female| 47|            Married|Black|2ndYearCollege|  15000|   Hospitality|    18000| 0.8333333333333334|\n",
      "|Female| 26|NeverMarried-Single|White|1stYearCollege|  25000|Administration|    32000|            0.78125|\n",
      "|Female| 49|            Married|White|4thYearCollege|  65000|    Healthcare|    67000| 0.9701492537313433|\n",
      "|Female| 64|            Married|White|1stYearCollege| 118000|      Business|    80000|              1.475|\n",
      "|Female| 58|            Married|White|4thYearCollege|  32000|   Hospitality|    18000| 1.7777777777777777|\n",
      "|Female| 61|            Married|White|    HighSchool|  38000|Administration|    32000|             1.1875|\n",
      "|Female| 43|            Married|White|2ndYearCollege|   5000|    Healthcare|    67000|0.07462686567164178|\n",
      "|Female| 29|NeverMarried-Single|Black|4thYearCollege|  14000|     Education|    47500|0.29473684210526313|\n",
      "|Female| 25|NeverMarried-Single|Black|    HighSchool|  17100|         Sales|    42000|0.40714285714285714|\n",
      "|Female| 28|NeverMarried-Single|Black|4thYearCollege|   1200|   Hospitality|    18000|0.06666666666666667|\n",
      "|Female| 17|NeverMarried-Single|White|    HighSchool|  10000|         Sales|    42000|0.23809523809523808|\n",
      "|Female| 26|NeverMarried-Single|White|2ndYearCollege|  37000|    Healthcare|    67000| 0.5522388059701493|\n",
      "|Female| 45|            Married|White|    HighSchool|  10400|         Sales|    42000|0.24761904761904763|\n",
      "|Female| 55|            Married|White|4thYearCollege|  30000|Administration|    32000|             0.9375|\n",
      "|Female| 52|            Married|White|4thYearCollege|  51000|    Healthcare|    67000| 0.7611940298507462|\n",
      "|Female| 26|NeverMarried-Single|White|4thYearCollege|   6000|           Law|    85000|0.07058823529411765|\n",
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#deriving the pay gap ratio column\n",
    "df3 = df3.withColumn(\"PayGapRatio\",df3['INCWAGE']/df3['M_MedWage'])\n",
    "\n",
    "#selecting only females\n",
    "df3 = df3.filter(df3['SEX']=='Female')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then reclassify\n",
    "pGRatioBucket = Bucketizer(splits=[0, 0.33, 0.66, 0.99, 1.0, float('Inf')],\n",
    "                       inputCol=\"PayGapRatio\",outputCol=\"payGapClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|payGapClass|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "|        1.0|\n",
      "|        4.0|\n",
      "|        3.0|\n",
      "|        2.0|\n",
      "+-----------+\n",
      "\n",
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+-------------------+-----------+\n",
      "|   SEX|AGE|              MARST| RACE|          EDUC|INCWAGE|     OCCRecode|M_MedWage|        PayGapRatio|payGapClass|\n",
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+-------------------+-----------+\n",
      "|Female| 31|NeverMarried-Single|White|4thYearCollege|  38500|      Business|    80000|            0.48125|        1.0|\n",
      "|Female| 37|          Separated|Black|    HighSchool|  18000|      Business|    80000|              0.225|        0.0|\n",
      "|Female| 58|            Married|Black|4thYearCollege| 344000|         Sales|    42000|   8.19047619047619|        4.0|\n",
      "|Female| 47|            Married|White|    HighSchool|  10000|     Education|    47500|0.21052631578947367|        0.0|\n",
      "|Female| 47|            Married|Black|2ndYearCollege|  15000|   Hospitality|    18000| 0.8333333333333334|        2.0|\n",
      "|Female| 26|NeverMarried-Single|White|1stYearCollege|  25000|Administration|    32000|            0.78125|        2.0|\n",
      "|Female| 49|            Married|White|4thYearCollege|  65000|    Healthcare|    67000| 0.9701492537313433|        2.0|\n",
      "|Female| 64|            Married|White|1stYearCollege| 118000|      Business|    80000|              1.475|        4.0|\n",
      "|Female| 58|            Married|White|4thYearCollege|  32000|   Hospitality|    18000| 1.7777777777777777|        4.0|\n",
      "|Female| 61|            Married|White|    HighSchool|  38000|Administration|    32000|             1.1875|        4.0|\n",
      "|Female| 43|            Married|White|2ndYearCollege|   5000|    Healthcare|    67000|0.07462686567164178|        0.0|\n",
      "|Female| 29|NeverMarried-Single|Black|4thYearCollege|  14000|     Education|    47500|0.29473684210526313|        0.0|\n",
      "|Female| 25|NeverMarried-Single|Black|    HighSchool|  17100|         Sales|    42000|0.40714285714285714|        1.0|\n",
      "|Female| 28|NeverMarried-Single|Black|4thYearCollege|   1200|   Hospitality|    18000|0.06666666666666667|        0.0|\n",
      "|Female| 17|NeverMarried-Single|White|    HighSchool|  10000|         Sales|    42000|0.23809523809523808|        0.0|\n",
      "|Female| 26|NeverMarried-Single|White|2ndYearCollege|  37000|    Healthcare|    67000| 0.5522388059701493|        1.0|\n",
      "|Female| 45|            Married|White|    HighSchool|  10400|         Sales|    42000|0.24761904761904763|        0.0|\n",
      "|Female| 55|            Married|White|4thYearCollege|  30000|Administration|    32000|             0.9375|        2.0|\n",
      "|Female| 52|            Married|White|4thYearCollege|  51000|    Healthcare|    67000| 0.7611940298507462|        2.0|\n",
      "|Female| 26|NeverMarried-Single|White|4thYearCollege|   6000|           Law|    85000|0.07058823529411765|        0.0|\n",
      "+------+---+-------------------+-----+--------------+-------+--------------+---------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3_buck = pGRatioBucket.setHandleInvalid(\"keep\").transform(df3)\n",
    "df3_buck.select(\"payGapClass\").distinct().show()\n",
    "\n",
    "df3_buck.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have reclassified the pay gap ratio, and this is a classification task, we proceed\n",
    "\n",
    "#onehotencoding\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"MARST\",\"RACE\",\"EDUC\",\"OCCRecode\"]\n",
    "num_cols = [\"AGE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building indexers and encoders for cat variables\n",
    "indexers = [StringIndexer(inputCol = c, outputCol=\"{0}_i\".format(c)) for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol = indexer.getOutputCol(), outputCol = \"{0}_e\".format(indexer.getOutputCol())) \n",
    "for indexer in indexers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assembler for cat\n",
    "assemblerCat = VectorAssembler(inputCols = [encoder.getOutputCol() for encoder in encoders], outputCol = \"cat\")\n",
    "#pipeline for cat\n",
    "pipelineCat = Pipeline(stages = indexers + encoders + [assemblerCat])\n",
    "df3_buck = pipelineCat.fit(df3_buck).transform(df3_buck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[SEX: string, AGE: int, MARST: string, RACE: string, EDUC: string, INCWAGE: string, OCCRecode: string, M_MedWage: string, PayGapRatio: double, payGapClass: double, EDUC_i: double, RACE_i: double, MARST_i: double, OCCRecode_i: double, EDUC_i_e: vector, RACE_i_e: vector, MARST_i_e: vector, OCCRecode_i_e: vector, cat: vector]>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df3_buck = df3_buck.withColumn(\"AGE\", df3_buck[\"AGE\"].cast(IntegerType()))\n",
    "df3_buck.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building assembler for num \n",
    "assemblerNum = VectorAssembler(inputCols = num_cols, outputCol = \"num\")\n",
    "pipelineNum = Pipeline(stages = [assemblerNum])\n",
    "df3_buck = pipelineNum.fit(df3_buck).transform(df3_buck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining cat and num assemblers\n",
    "assembler = VectorAssembler(inputCols = [\"cat\", \"num\"], outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#into the pipeline\n",
    "pipeline = Pipeline(stages = [assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new df with both cat and num as features\n",
    "df_temp = pipeline.fit(df3_buck).transform(df3_buck)\n",
    "df_f = df_temp.select(\"features\",\"payGapClass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all features into one vector named features (without Age added in)\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training and test sets\n",
    "train_data,test_data = df_f.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Classifiers\n",
    "#importing relevant ones\n",
    "from pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use defaults to make the comparison fair\n",
    "dtc = DecisionTreeClassifier(labelCol=\"payGapClass\",featuresCol=\"features\")\n",
    "rfc = RandomForestClassifier(labelCol='payGapClass',featuresCol='features')\n",
    "gbt = GBTClassifier(labelCol='payGapClass',featuresCol='features', maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o300.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 852, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1327)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:291)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:53)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:167)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:60)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-306c11426ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrfc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdtc_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgbt_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgbt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \"\"\"\n\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.1.1-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o300.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 88.0 failed 1 times, most recent failure: Lost task 0.0 in stage 88.0 (TID 852, localhost, executor driver): java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1925)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1951)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1327)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:112)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:105)\n\tat org.apache.spark.ml.regression.DecisionTreeRegressor.train(DecisionTreeRegressor.scala:125)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.boost(GradientBoostedTrees.scala:291)\n\tat org.apache.spark.ml.tree.impl.GradientBoostedTrees$.run(GradientBoostedTrees.scala:53)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:167)\n\tat org.apache.spark.ml.classification.GBTClassifier.train(GBTClassifier.scala:60)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:96)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalArgumentException: requirement failed: GBTClassifier was given dataset with invalid label 2.0.  Labels must be in {0,1}; note that GBTClassifier currently only supports binary classification.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:154)\n\tat org.apache.spark.ml.classification.GBTClassifier$$anonfun$1.apply(GBTClassifier.scala:152)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:409)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1005)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:936)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:996)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:700)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:334)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:285)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "rfc_model = rfc.fit(train_data)\n",
    "dtc_model = dtc.fit(train_data)\n",
    "gbt_model = gbt.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "dtc_predictions = dtc_model.transform(test_data)\n",
    "gbt_predictions = gbt_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then we evaluate using the multi class classification evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"payGapClass\", predictionCol=\"prediction\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "gbt_acc = acc_evaluator.evaluate(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('An ensemble Random Forest Classifier has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))\n",
    "print('A single Decision Tree Classifier has an accuracy of: {0:2.2f}%'.format(dtc_acc*100))\n",
    "print('An ensemble Gradient Boosted Classifier has an accuracy of: {0:2.2f}%'.format(gbt_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
